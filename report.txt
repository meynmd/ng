commands:

cat ex2-data/test.txt | sed -e 's/ /_/g;s/\(.\)/\1 /g' | awk '{printf("<s> %s </s>\n", $0)}' | carmel -slibOEWk 1 remove-vowels.fst > test.novowels

cat test.novowels | carmel -sriIEWbk 10 tri.norm remove-vowels.fst



CS 519-001 Natural Language Processing, Spring 2017
EX 2: Language Models and Entropy

1. Name your models:
    unigram.wfsa
    bigram.wfsa
    trigram.wfsa



2.  What are the corpus probabilities your three models assign to the test data?  What are the respective
(cross) entropies?

    Unigram:    per-symbol entropy  4.115
        product of probs=e^-27039.440833621,
        probability=2^-39009.7
        per-input-symbol-perplexity(N=9480)=2^4.11494
        per-line-perplexity(N=100)=2^390.097

    Bigram:     per-symbol entropy  3.492
        product of probs=e^-22942.8845645149,
        probability=2^-33099.6
        per-input-symbol-perplexity(N=9480)=2^3.49152
        per-line-perplexity(N=100)=2^330.996

    Trigram:    per-symbol entropy  2.986
        product of probs=e^-19624.1796699237,
        probability=2^-28311.7
        per-input-symbol-perplexity(N=9480)=2^2.98647
        per-line-perplexity(N=100)=2^283.117



3.  What are the final sizes of your WFSAs in states and transitions.

    unigram.wfsa:   Number of states in result: 3
                    Number of arcs in result: 29

    bigram.wfsa:    Number of states in result: 30
                    Number of arcs in result: 785

    trigram.wfsa:   Number of states in result: 759
                    Number of arcs in result: 21197



4.  Include a description of your smoothing method.  Your description should include the algorithms, how
you used the held-out data, and what (if any) experiments you did before settling on your solution.

After trying add-1 smoothing to get started, I changed the method to less-than-one smoothing, but was not
satisfied with the results, in the neighborhood of 3.7 cross-entropy. I then implemented a basic Good-Turing
smoothing to assign probability to observations with zero count. This got me closer to entropy of 3, but I
had to make some further modifications to get below that. First, I applied Good-Turing smoothing to all counts
<= k for some fixed k, but then decided to let the program choose k adaptively for each (n-1) sequence. The
choice of k now depends on (1) the variance in counts following this sequence, and (2) how many times the
sequence itself has been seen. This is based on the intuition that we should have more confidence in what we
have learned from looking at what follows a sequence we have seen 100 times than one we have seen once or
twice. Specifically the algorithm looks like this:

for each (sequence, char-count) in counter
    seen-score <- sequence count / max sequence count
    sequence-weight <- exp(-2 * seen-score)
    variance-weight <- stdev(char-count.values)
    k <- ceiling(log(variance-weight + 1) * sequence-weight)

    if sum of counts in char-count = 0, then
        prob of each char given sequence <- 1 / |all possible chars|
    else
        for each (char, count) in char-count
            if count > k, then
                prob of char given sequence <- count / sum(counts for sequence)
            else
                if




5.  Include  a  sketch  drawing  of  your  3-gram  model  in  sufficient  detail  that  someone  could
replicate  it. Please consider this carefully – examine your drawing after you have drawn it and evaluate
whether someone (not you) could build the same WFSA you have built.  We will consider it in the same light.
1
Important:
•
There is a special start symbol
<s>
and special end symbol
</s>
around every sentence, so that we can
model for example the fact that letter
x
normally does not start a sentence, and the letter
z
normally
does not end a sentence (we omit punctuations).
Here is an example unigram WFSA on two characters (
ab.wfsa
):
2
(0 (1 <s> 1))
(1 (1 a 0.8))
(1 (1 b 0.1))
(1 (2 </s> 0.1))
Note that
<s>
is
not
a token in the language,  while
</s>
is.  That is why the probability of
<s>
is
always  1,  and  the  probability  of
</s>
is  normalized  with  other  characters.   We’ve  also  provided  an
example
uni.wfsa
.
•
To  ensure  that  your  WFSA  represents  a  legitimate  probability  distribution,  you  should  normalize
it  with
carmel -Hjn wfsa > wfsa.norm
(
-j
for  joint  prob.  model,  i.e.,  WFSA-style  instead  of  the
conditional WFST style).  We will do this (before testing your models) in any case.  You can try this
on the above sample and it will give you the same WFSA since it’s already normalized.
•
To ensure that your WFSA represents a legitimate probability distribution, you should have a single
final state with no exiting transitions.  The above sample also demonstrates this.
2  Using Language Models
Language models are often used for generation, prediction, and decoding in a noisy-channel framework.
1.  Random generation from
n
-gram models.
Use
carmel -GI 20 <your_wfsa>
to stochastically generate character sequences.  Show the results.
Do these results make sense?  (For example, you can do the same on the above sample WFSA, and you
will see the proportion of
a
:
b
is indeed about 8:1.)
2.  Restoring vowels.
Just as in HW1, we can decode text without vowels.  Try doing that experiment on
test.txt
with the
language models you trained. What’s your command line? What are your accuracy results? Include the
input file
test.txt.novowels
and the result files
test.txt.vowel_restored.{uni,bi,tri}
.  (Hint:
you can use
sed -e 's/[aeiou]//g'
to remove vowels).
3.  Restoring spaces.
Similarly, we can remove the spaces and try to restore them with the help of language models.
Try doing that experiment on
test.txt
with the language models you trained.  What’s your command
line?  What are your accuracy results?  Include the input file
test.txt.nospaces
and the result files
test.txt.space_restored.{uni,bi,tri}
.  (Hint:  you can use
sed -e 's/ //g'
to remove spaces).
Finally, decode the following two sentences with your models:
therestcanbeatotalmessandyoucanstillreaditwithoutaproblem
thisisbecausethehumanminddoesnotreadeveryletterbyitselfbutthewordasawhole.
4.  Which of the two decoding problems is easier?  Write a paragraph of observations you made in these
experiments.
2
